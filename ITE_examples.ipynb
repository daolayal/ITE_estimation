{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegoolaya/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/diegoolaya/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from econml.dml import ForestDMLCateEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pylift.eval import UpliftEval\n",
    "from econml.metalearners import TLearner, SLearner, XLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hillstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv\")\n",
    "data[\"segment\"] = data[\"segment\"].astype(\"category\")\n",
    "data[\"history_segment\"] = data[\"history_segment\"].astype(\"category\")\n",
    "data[\"zip_code\"] = data[\"zip_code\"].astype(\"category\")\n",
    "data[\"channel\"] = data[\"channel\"].astype(\"category\")\n",
    "data_size = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_hillstrom(data):\n",
    "    context = data[[\"recency\", \"history_segment\", \"history\", \"mens\", \"womens\", \"zip_code\", \"newbie\", \"channel\"]].copy()\n",
    "    treatment = data[\"segment\"]\n",
    "    outcome = data[\"visit\"]\n",
    "\n",
    "    one_hot_hs = pd.get_dummies(context[\"history_segment\"], prefix=\"hs\")\n",
    "    one_hot_zc = pd.get_dummies(context[\"zip_code\"], prefix=\"zc\")\n",
    "    one_hot_c = pd.get_dummies(context[\"channel\"], prefix=\"c\")\n",
    "\n",
    "    context = pd.concat([context[[\"recency\", \"history\", \"mens\", \"womens\", \"newbie\"]], one_hot_hs, one_hot_zc, one_hot_c], axis=1)\n",
    "\n",
    "    return (context.values, treatment.values, outcome.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, data, data_parser, train_ratio=.5,validation_ratio = .25,test_ratio = .25):\n",
    "        self.data = shuffle(data)\n",
    "        self.train_ratio = train_ratio\n",
    "        self.validation_ratio = validation_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.data_parser = data_parser\n",
    "        \n",
    "        self.data['strat'] = self.data[\"segment\"].astype(str) + self.data[\"visit\"].astype(str)\n",
    "        self.data_w = self.data.loc[self.data['segment'].isin([\"Womens E-Mail\",'No E-Mail'])]\n",
    "        self.data_m = self.data.loc[self.data['segment'].isin([\"Mens E-Mail\",'No E-Mail'])]\n",
    "        \n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=(1-train_ratio), random_state=0, stratify=self.data[['strat']])\n",
    "        self.validation_data, self.test_data = train_test_split(self.test_data, test_size = self.test_ratio/(self.test_ratio+self.validation_ratio), random_state=0, stratify=self.test_data[['strat']])\n",
    "\n",
    "        self.train_data_w, self.test_data_w = train_test_split(self.data_w, test_size=(1-train_ratio), random_state=0, stratify=self.data_w[['strat']])\n",
    "        self.validation_data_w, self.test_data_w = train_test_split(self.test_data_w, test_size = self.test_ratio/(self.test_ratio+self.validation_ratio), random_state=0, stratify=self.test_data_w[['strat']])\n",
    "        \n",
    "        self.c_tr, self.t_tr, self.o_tr = self.data_parser(self.train_data_w)\n",
    "        self.t_tr = self.t_tr.codes - 1\n",
    "        self.c_va, self.t_va, self.o_va = self.data_parser(self.validation_data_w)\n",
    "        self.t_va = self.t_va.codes - 1\n",
    "        self.c_te, self.t_te, self.o_te = self.data_parser(self.test_data_w)\n",
    "        self.t_te = self.t_te.codes - 1\n",
    "\n",
    "        self.td_tr = np.concatenate((self.c_tr, np.array([self.t_tr]).T, np.array([self.o_tr]).T), axis=1)\n",
    "        self.td_va = np.concatenate((self.c_va, np.array([self.t_va]).T, np.array([self.o_va]).T), axis=1)\n",
    "        self.td_te = np.concatenate((self.c_te, np.array([self.t_te]).T, np.array([self.o_te]).T), axis=1)\n",
    "        \n",
    "        self.cols = [f'x{i}' for i in range(self.c_tr.shape[1])]\n",
    "        self.non_c = ['t', 'y']\n",
    "        self.cols.extend(self.non_c)\n",
    "        self.df_tr = pd.DataFrame(data=self.td_tr, columns=self.cols)\n",
    "        self.df_va = pd.DataFrame(data=self.td_va, columns=self.cols)\n",
    "        self.df_te = pd.DataFrame(data=self.td_te, columns=self.cols)\n",
    "        self.x = [variable for variable in list(self.df_tr.columns) if variable not in self.non_c] \n",
    "        \n",
    "        self.param_tuning = {\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'n_estimators' : [100,200,300],\n",
    "            'random_state': [0]\n",
    "        }\n",
    "        \n",
    "    def CF(self):\n",
    "        est = ForestDMLCateEstimator(model_y = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1),\n",
    "                                     model_t = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1),\n",
    "                                     discrete_treatment=True,\n",
    "                                     n_estimators= 100,\n",
    "                                     n_jobs = -1,\n",
    "                                     random_state = 0)\n",
    "        \n",
    "        est.fit(self.df_tr['y'].to_numpy(), self.df_tr['t'].to_numpy(),self.df_tr[self.x].to_numpy(), inference = 'blb')\n",
    "        \n",
    "        pre = est.effect(self.df_va[self.x].to_numpy())\n",
    "        \n",
    "        upev = UpliftEval(self.df_va['t'], self.df_va['y'], pre)\n",
    "\n",
    "        return upev\n",
    "    \n",
    "    def meta_learners(self, learner):\n",
    "\n",
    "        if learner == 'T_learner':\n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = TLearner(model)\n",
    "\n",
    "        elif learner == 'S_learner':\n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = SLearner(model)\n",
    "            \n",
    "        elif learner == 'X_learner':            \n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            propensity_model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = XLearner(models = model, propensity_model=propensity_model)\n",
    "\n",
    "        est.fit(self.df_tr['y'].to_numpy(), self.df_tr['t'].to_numpy(), self.df_tr[self.x].to_numpy())\n",
    "   \n",
    "        pre = est.effect(self.df_va[self.x].to_numpy())\n",
    "\n",
    "        upev = UpliftEval(self.df_va['t'], self.df_va['y'], pre)\n",
    "\n",
    "        return upev    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = Train(data,parse_data_hillstrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_CF = t.CF()\n",
    "estimated_CF.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_CF.plot(plot_type='aqini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_metalearners = t.meta_learners(learner = 'T_learner')\n",
    "estimated_metalearners.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_metalearners.plot(plot_type='aqini')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"CS.csv\",sep = ';')\n",
    "\n",
    "def parse_data_chile(data):\n",
    "    context = data[[\"year\", \"sen_tenure\", \"jerarquia\", \"homophily\", \"indegree\", \"strengthoftie\", \"NAT\"]].copy()\n",
    "    treatment = data[\"CAMP\"]\n",
    "    data[\"Y\"] = np.where(data[\"Y\"]==1,1,0)\n",
    "    outcome = data[\"Y\"]\n",
    "    return (context.values, treatment.values, outcome.values)\n",
    "\n",
    "\n",
    "class Train_chile:\n",
    "    def __init__(self, data, data_parser, train_ratio=.7):\n",
    "        self.data = shuffle(data)\n",
    "        self.train_ratio = train_ratio\n",
    "        self.data_parser = data_parser\n",
    "        \n",
    "        self.data['strat'] = self.data[\"CAMP\"].astype(str) + self.data[\"Y\"].astype(str)\n",
    "        \n",
    "        self.train_data, self.test_data = train_test_split(self.data, test_size=(1-train_ratio), random_state=0, stratify=self.data[['strat']])\n",
    "\n",
    "        self.c_tr, self.t_tr, self.o_tr = self.data_parser(self.train_data)\n",
    "        self.c_te, self.t_te, self.o_te = self.data_parser(self.test_data)\n",
    "        \n",
    "        self.td_tr = np.concatenate((self.c_tr, np.array([self.t_tr]).T, np.array([self.o_tr]).T), axis=1)\n",
    "        self.td_te = np.concatenate((self.c_te, np.array([self.t_te]).T, np.array([self.o_te]).T), axis=1)\n",
    "        \n",
    "        self.cols = [f'x{i}' for i in range(self.c_tr.shape[1])]\n",
    "        self.non_c = ['t', 'y']\n",
    "        self.cols.extend(self.non_c)\n",
    "        self.df_tr = pd.DataFrame(data=self.td_tr, columns=self.cols)\n",
    "        self.df_te = pd.DataFrame(data=self.td_te, columns=self.cols)\n",
    "        self.x = [variable for variable in list(self.df_tr.columns) if variable not in self.non_c] \n",
    "        \n",
    "        self.param_tuning = {\n",
    "            'max_depth': [3, 5, 7, 10],\n",
    "            'n_estimators' : [100,200,300],\n",
    "            'random_state': [0]\n",
    "        }\n",
    "        \n",
    "    def CF(self):\n",
    "        est = ForestDMLCateEstimator(model_y = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1),\n",
    "                                     model_t = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1),\n",
    "                                     discrete_treatment=True,\n",
    "                                     n_estimators= 100,\n",
    "                                     n_jobs = -1,\n",
    "                                     random_state = 0)\n",
    "        \n",
    "        est.fit(self.df_tr['y'].to_numpy(), self.df_tr['t'].to_numpy(),self.df_tr[self.x].to_numpy(), inference = 'blb')\n",
    "        \n",
    "        pre = est.effect(self.df_te[self.x].to_numpy())\n",
    "        \n",
    "        upev = UpliftEval(self.df_te['t'], self.df_te['y'], pre)\n",
    "\n",
    "        return upev\n",
    "    \n",
    "    def meta_learners(self, learner):\n",
    "\n",
    "        if learner == 'T_learner':\n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = TLearner(model)\n",
    "\n",
    "        elif learner == 'S_learner':\n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = SLearner(model)\n",
    "            \n",
    "        elif learner == 'X_learner':            \n",
    "            model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            propensity_model = GridSearchCV(estimator = RandomForestClassifier(), param_grid = self.param_tuning,cv = 5,n_jobs = -1,verbose = 1)\n",
    "            est = XLearner(models = model, propensity_model=propensity_model)\n",
    "\n",
    "        est.fit(self.df_tr['y'].to_numpy(), self.df_tr['t'].to_numpy(), self.df_tr[self.x].to_numpy())\n",
    "   \n",
    "        pre = est.effect(self.df_te[self.x].to_numpy())\n",
    "\n",
    "        upev = UpliftEval(self.df_te['t'], self.df_te['y'], pre)\n",
    "\n",
    "        return upev    \n",
    "\n",
    "t_chile = Train_chile(data,parse_data_chile)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimated_CF = t_chile.CF()\n",
    "estimated_CF.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_CF.plot(plot_type='aqini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_metalearners = t_chile.meta_learners(learner = 'T_learner')\n",
    "estimated_metalearners.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_metalearners.plot(plot_type='aqini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_metalearners = t_chile.meta_learners(learner = 'S_learner')\n",
    "estimated_metalearners.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_metalearners.plot(plot_type='aqini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_metalearners = t_chile.meta_learners(learner = 'X_learner')\n",
    "estimated_metalearners.calc(plot_type='aqini', n_bins=100)\n",
    "estimated_metalearners.plot(plot_type='aqini')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
